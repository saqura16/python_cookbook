{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 问题:\n",
    "我们有一个字符串，想从左到右将它解析为标记流（stream of tokens）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 解决方案:\n",
    "假设有如下的字符串文本:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'foo = 23 + 42 * 10' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要对字符串做分词处理，需要做的不仅仅只是匹配模式。我们还需要有某种方法来识别出模式的类型。例如，我们可能想将字符串转换为如下的序列对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =[('NAME', 'foo'), ('EQ','='), ('NUM', '23'), ('PLUS','+'),('NUM', '42'), ('TIMES', '*'), ('NUM', '10')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要完成这样的分词处理，第一步是定义出所有可能的标记，包括空格。这可以通过正则表达式中的命名捕获组来实现，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "NAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)' \n",
    "NUM = r'(?P<NUM>\\d+)' \n",
    "PLUS = r'(?P<PLUS>\\+)' \n",
    "TIMES = r'(?P<TIMES>\\*)'\n",
    "EQ= r'(?P<EQ>=)' \n",
    "WS= r'(?P<WS>\\s+)' \n",
    "master_pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这些正则表达式模式中，形如(?P<\\name>)这样的约定是用来将名称分配给该模式的。这个我们稍后会用到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们使用模式对象的scanner()方法来完成分词操作。该方法会创建一个扫描对象，在给定的文本中重复调用match()，一次匹配一个模式。下面这个交互式示例展示了扫描对象是如何工作的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='foo'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scanner = master_pat.scanner('foo = 42') \n",
    "scanner.match() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='NAME', value='foo')\n",
      "Token(type='WS', value=' ')\n",
      "Token(type='EQ', value='=')\n",
      "Token(type='WS', value=' ')\n",
      "Token(type='NUM', value='42')\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple \n",
    "Token = namedtuple('Token', ['type','value']) \n",
    "\n",
    "def generate_tokens(pat, text):     \n",
    "    scanner = pat.scanner(text)     \n",
    "    for m in iter(scanner.match, None):\n",
    "         yield Token(m.lastgroup, m.group()) \n",
    "\n",
    "# Example use \n",
    "for tok in generate_tokens(master_pat, 'foo = 42'):\n",
    "    print(tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想以某种方式对标记流做过滤处理，要么定义更多的生成器函数，要么就用生成器表达式。例如，下面的代码告诉我们如何过滤掉所有的空格标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='NAME', value='foo')\n",
      "Token(type='EQ', value='=')\n",
      "Token(type='NUM', value='23')\n",
      "Token(type='PLUS', value='+')\n",
      "Token(type='NUM', value='42')\n",
      "Token(type='TIMES', value='*')\n",
      "Token(type='NUM', value='10')\n"
     ]
    }
   ],
   "source": [
    "tokens = (tok for tok in generate_tokens(master_pat, text) if tok.type != 'WS') \n",
    "for tok in tokens:\n",
    "         print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于更加高级的文本解析，第一步往往是分词处理。要使用上面展示的扫描技术，有几个重要的细节需要牢记于心。第一，对于每个可能出现在输入文本中的文本序列，都要确保有一个对应的正则表达式模式可以将其识别出来。如果发现有任何不能匹配的文本，扫描过程就会停止。这就是为什么有必要在上面的示例中指定空格标记（WS）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些标记在正则表达式（即re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))）中的顺序同样也很重要。当进行匹配时，re模块会按照指定的顺序来对模式做匹配。因此，如果碰巧某个模式是另一个较长模式的子串时，就必须确保较长的那个模式要先做匹配。示例如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nLT = r'(?P<LT><)'\\nLE = r'(?P<LE><=)'\\nEQ = r'(?P<EQ>=)' \\nmaster_pat = re.compile('|'.join([LE, LT, EQ])) # Correct\\n# master_pat = re.compile('|'.join([LT, LE, EQ])) # Incorrect \\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "LT = r'(?P<LT><)'\n",
    "LE = r'(?P<LE><=)'\n",
    "EQ = r'(?P<EQ>=)' \n",
    "master_pat = re.compile('|'.join([LE, LT, EQ])) # Correct\n",
    "# master_pat = re.compile('|'.join([LT, LE, EQ])) # Incorrect \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第2个模式是错误的（注释掉的那一行），因为这样会把文本'<='匹配为LT（'<'）紧跟着EQ（'='），而没有匹配为单独的标记LE（'<='），这与我们的本意不符。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后也最重要的是，对于有可能形成子串的模式要多加小心。例如，假设有如下两种模式:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT = r'(P<PRINT>print)' \n",
    "NAME  = r'(P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)' \n",
    "master_pat = re.compile('|'.join([PRINT, NAME])) \n",
    "for tok in generate_tokens(master_pat, 'printer'):\n",
    "    print(tok)\n",
    "\n",
    "\n",
    "# Outputs : \n",
    "# Token(type='PRINT', value='print') \n",
    "# Token(type='NAME', value='er') \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e68ebf596df7713ab296bba418491aae41f185aff501f4839864b40868f87cf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
